import sys, os
import shutil

import argparse

import torch
import json
from tqdm import tqdm
import shortuuid

# llava
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria

## cogvlm
from transformers import AutoModelForCausalLM, LlamaTokenizer
from accelerate import init_empty_weights, load_checkpoint_and_dispatch

# minigptv4

import torch.backends.cudnn as cudnn
from minigpt4.common.config import Config
from minigpt4.common.registry import registry
from minigpt4.conversation.conversation import Conversation, SeparatorStyle, Chat

from minigpt4.datasets.builders import *
from minigpt4.models import *
from minigpt4.processors import *
from minigpt4.runners import *
from minigpt4.tasks import *

import warnings
warnings.filterwarnings("ignore")

import pandas as pd
from PIL import Image
import random

import sys, os
import shutil
import argparse
import random
import pandas as pd
import glob
import clip

def load_image(image_path):
    image = Image.open(image_path).convert('RGB')
    return image

def get_test_sample_list(args):
    split_file = open(args.data_split_file, 'r', encoding='utf-8')
    split_list = []
    for line in split_file.readlines():
        dic = json.loads(line)
        split_list.append(dic)
        
    for item in split_list:
        if item.get(args.scenario):
            test_img_list = item[args.scenario]['test']
            break
        else:
            pass

    return test_img_list

@torch.no_grad()
def get_clip_embedding(clip_model, clip_preprocess, image_file, query):
    image = clip_preprocess(load_image(image_file)).unsqueeze(0).cuda()
    text = clip.tokenize(query).cuda()
    image_feature = clip_model.encode_image(image)
    text_feature = clip_model.encode_text(text)

    embedding = torch.cat((image_feature,text_feature),dim=-1)
    return torch.nn.functional.normalize(embedding, dim=-1)


def retrival_defense_prompt(prompt_pool,image_pool, retrival_type="random", sample_embedding=None, embedding_pool=None):
    if retrival_type == "random":
        index = random.randint(0, len(prompt_pool)-1)
        final_defense_prompt = prompt_pool[index]
        final_image = image_pool[index]
        best_similarity = 0

    elif retrival_type == "sample-wise":
        assert sample_embedding is not None
        assert embedding_pool is not None
        similarity = (sample_embedding.float() @ embedding_pool.float().t()).squeeze()

        best_indice_list = torch.nonzero(similarity == similarity.max())
        if best_indice_list.shape[0] ==1:
            random_best_index = best_indice_list
        else:
            random_best_index = random.choice(best_indice_list)
        final_defense_prompt = prompt_pool[random_best_index]
        final_image = image_pool[random_best_index]
        best_similarity = similarity[random_best_index].item()
    else:
        raise ValueError("retrival_type should be either'scenario-wise' or'sample-wise'.")
    return final_defense_prompt, final_image , best_similarity 

def read_file_to_string(filename):
    with open(filename, 'r') as file:
        content = file.read()
    return content

def load_image_paths_from_folder(folder_path):
    image_paths = []
    for filename in os.listdir(folder_path):
        if filename.endswith(".jpg") or filename.endswith(".png") \
                or filename.endswith(".JPEG"):
            image_path = os.path.join(folder_path, filename)
            image_paths.append(image_path)
    return image_paths


def get_all_defense_example_pool(args, clip_model, clip_preprocess, table_dir):
    scenerio_list = [ "01-Illegal_Activitiy",
            "02-HateSpeech", "03-Malware_Generation", "04-Physical_Harm",
            "05-EconomicHarm", "06-Fraud", "07-Sex", "08-Political_Lobbying", 
            "09-Privacy_Violence", "10-Legal_Opinion", "11-Financial_Advice", "12-Health_Consultation", "13-Gov_Decision" ]
    embedding_pool = None
    image_pool  = []
    final_defense_pool = []
    for scene in scenerio_list:
        table_file_list = glob.glob(os.path.join( table_dir, scene, "wandb", args.wandb_subdir ,"files/final_table.csv"))
        if len(table_file_list)==0:
            print("No table file found")
            return
        table_name = table_file_list[0]
        full_df = pd.read_csv(table_name)

        temp_df = full_df

        success_df = temp_df[temp_df["final_judge_scores"]==1]

        temp_image_pool =  success_df["image"].tolist()
        temp_defense_pool = success_df["defense_prompt_list"].tolist()
        temp_query_pool =  success_df["query"].tolist()
        
        image_pool += temp_image_pool
        final_defense_pool += temp_defense_pool
        
        for i in tqdm(range(len(temp_image_pool))):
            image_file = temp_image_pool[i]
            query = temp_query_pool[i]
            embedding = get_clip_embedding(clip_model, clip_preprocess, image_file, query)
            if embedding_pool is None:
                embedding_pool = torch.zeros(0,embedding.shape[1]).to(embedding.device)
            embedding_pool = torch.cat((embedding_pool,embedding),dim=0)
        
    return final_defense_pool, embedding_pool, image_pool 


def VQA(CONV_VISION , chat, image_file, prompt):
    image_file = image_file.convert('RGB')
    prompt = prompt[:2000]

    # ask
    chat_state = CONV_VISION.copy()
    chatbot = []
    img_list = []
    llm_message = chat.upload_img(image_file, chat_state, img_list)

    chat.ask(prompt, chat_state)

    chatbot = chatbot + [[prompt, None]]

    # answer
    if len(img_list) > 0:
        if not isinstance(img_list[0], torch.Tensor):
            chat.encode_img(img_list)

    llm_message = chat.answer(conv=chat_state,
                              img_list=img_list,
                              temperature=0.2,
                              max_new_tokens=500,
                              max_length=2048)[0]

    return llm_message

def Attack_MiniGPT(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
    
    print("load the test image list")
    test_image_list = get_test_sample_list(args)

    answers_file = os.path.expanduser(args.result_path)
    
    json_file_name = "{scen}.json".format(scen=args.scenario)
    query_file_path = os.path.join(args.prompt_dir, json_file_name)
    

    image_folder = os.path.join(args.image_folder , args.scenario , args.query_type)
    
    with open(query_file_path) as f:
        query_list = json.load(f)


    # Model
    cfg = Config(args)
    device = 'cuda:{}'.format(args.gpu_id)

    model_config = cfg.model_cfg
    model_config.device_8bit = args.gpu_id
    model_cls = registry.get_model_class(model_config.arch)
    model = model_cls.from_config(model_config).to(device)

    vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train
    vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)
    model = model.eval()
    
    print('[Initialization Finished]\n')
    CONV_VISION = Conversation(
        system="",
        roles=(r"<s>[INST] ", r" [/INST]"),
        messages=[],
        offset=2,
        sep_style=SeparatorStyle.SINGLE,
        sep="",
    )

    chat = Chat(model, vis_processor, device=device)

    
    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "w")

    print("start to get defense example pool!")
    defense_prompt_pool, embedding_pool, image_pool = get_all_defense_example_pool(args, clip_model, clip_preprocess, args.table_dir)
    print(" END !")


    total = len(query_list)
    print("Total files:",total)

    with tqdm(total=total) as par:
        for i in query_list:
            image_file = i+".jpg"
            full_image_path = os.path.join(image_folder, image_file)

            if full_image_path in test_image_list:
                if args.query_type == 'SD':
                    query = query_list[i]['Rephrased Question(SD)']
                else:
                    query = query_list[i]['Rephrased Question']
                    
                if args.retrival_type == "random":
                    defense_query, best_simiarity = retrival_defense_prompt(defense_prompt_pool, retrival_type="random")
                    
                elif args.retrival_type == "sample-wise":
                    sample_embedding = get_clip_embedding(clip_model, clip_preprocess,full_image_path, query)
                    defense_query, best_simiarity = retrival_defense_prompt(defense_prompt_pool, retrival_type= "sample-wise", sample_embedding=sample_embedding, embedding_pool=embedding_pool)

                image = Image.open(full_image_path).convert('RGB')

                text_prompt =  query + defense_query + query
                
                response = VQA(CONV_VISION , chat, image, text_prompt)

                ans_id = shortuuid.uuid()
                ans_file.write(json.dumps({"text": response,
                                "defense_query":defense_query,
                                "best_simiarity": best_simiarity,   
                                "image_file": full_image_path,
                                "answer_id": ans_id,
                                "model_id": "minigptv2",
                                "metadata": {}}) + "\n")
                ans_file.flush()
            else:
                print("pass")

            par.update(1)
    ans_file.close()

def Attack_CogVLM(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
    
    print("load the test image list")
    test_image_list = get_test_sample_list(args)
    answers_file = os.path.expanduser(args.result_path)

    json_file_name = "{scen}.json".format(scen=args.scenario)
    query_file_path = os.path.join(args.prompt_dir, json_file_name)


    with open(query_file_path) as f:
        query_list = json.load(f)


    image_folder = os.path.join(args.image_folder , args.scenario , args.query_type)
    
    # build model
    tokenizer = LlamaTokenizer.from_pretrained(args.llm_path) #
    model_name = os.path.basename(args.llm_path)
    with init_empty_weights():
        model = AutoModelForCausalLM.from_pretrained(
            args.model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True)

    model = load_checkpoint_and_dispatch(
                    model,
                    args.model_path,
                    device_map="auto",
                    no_split_module_classes=['CogVLMDecoderLayer', 'TransformerLayer'])
    
    model = model.eval()


    print("start to get defense example pool!")
    defense_prompt_pool, embedding_pool, image_pool  = get_all_defense_example_pool(args, clip_model, clip_preprocess, args.table_dir)
    print(" END !")
    # answer file
    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "w")
    total = len(query_list)
    print("Total files:" , total)

    with tqdm(total=total) as par:
        for i in query_list:
            image_file = i+".jpg"
            full_image_path = os.path.join(image_folder, image_file)
            if full_image_path in test_image_list:
                if args.query_type == 'SD':
                    query = query_list[i]['Rephrased Question(SD)']
                else:
                    query = query_list[i]['Rephrased Question']

                if args.retrival_type == "random":
                    defense_query,anchor_image_path,  best_simiarity = retrival_defense_prompt(defense_prompt_pool, image_pool,  retrival_type= "random")
                    
                elif args.retrival_type == "sample-wise":
                    sample_embedding = get_clip_embedding(clip_model, clip_preprocess, full_image_path, query)
                    defense_query, anchor_image_path, best_simiarity = retrival_defense_prompt(defense_prompt_pool, image_pool, retrival_type= "sample-wise", sample_embedding=sample_embedding, embedding_pool=embedding_pool)
                    

                query =  query + defense_query


                image = Image.open(full_image_path).convert('RGB')
                inputs = model.build_conversation_input_ids(tokenizer, query=query, history=[], images=[image]) # chat mode
                input_ids = inputs['input_ids'].unsqueeze(0).to('cuda')

                with torch.no_grad():
                    outputs = model.generate(
                            input_ids = input_ids,
                            images = [[inputs['images'][0].to('cuda').to(torch.bfloat16)]],
                            token_type_ids = inputs['token_type_ids'].unsqueeze(0).to('cuda'),
                            attention_mask = inputs['attention_mask'].unsqueeze(0).to('cuda'),
                            do_sample = True,
                            temperature = args.temperature,
                            top_p = args.top_p,
                            top_k = args.top_k,
                            max_length = args.max_length
                    )

                    outputs = outputs[:, input_ids.shape[1]:]

                respone = tokenizer.decode(outputs[0])
                ans_id = shortuuid.uuid()

                ans_file.write(json.dumps({"text": respone,
                                "defense_query":defense_query,
                                "anchor_image_path":anchor_image_path,
                                "best_simiarity": best_simiarity,   
                                "image_file": full_image_path,
                                "answer_id": ans_id,
                                "model_id": model_name,
                                "metadata": {}}) + "\n")
                ans_file.flush()
            else:
                print("pass")                
            par.update(1)

    ans_file.close()

def Attack_LLaVA(args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    clip_model, clip_preprocess = clip.load("ViT-B/32", device=device)
    
    print("load the test image list")
    test_image_list = get_test_sample_list(args)
    
    answers_file = os.path.expanduser(args.result_path)
    json_file_name = "{scen}.json".format(scen=args.scenario)
    query_file_path = os.path.join(args.prompt_dir, json_file_name)

    image_folder = os.path.join(args.image_folder , args.scenario , args.query_type)

    with open(query_file_path) as f:
        query_list = json.load(f)
    

    # Model
    disable_torch_init()
    model_path = os.path.expanduser(args.model_path)
    model_name = get_model_name_from_path(model_path)

    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name)

    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "w")

    if "llama-2" in model_name.lower():
        conv_mode = "llava_llama_2"
    elif "v1" in model_name.lower():
        conv_mode = "llava_v1"
    elif "mpt" in model_name.lower():
        conv_mode = "mpt"
    else:
        conv_mode = "llava_v0"

    if args.conv_mode is None or conv_mode == args.conv_mode:
                args.conv_mode = conv_mode

    print("start to get defense example pool!")
    defense_prompt_pool, embedding_pool,image_pool = get_all_defense_example_pool(args, clip_model, clip_preprocess, args.table_dir) 
    print(" END !")


    total = len(query_list)
    print("Total files:",total)

    
    with tqdm(total=total) as par:
        for i in query_list:
            image_file = i+".jpg"
            full_image_path = os.path.join(image_folder, image_file)
            if full_image_path in test_image_list:
                if args.query_type == 'SD':
                    query = query_list[i]['Rephrased Question(SD)']
                else:
                    query = query_list[i]['Rephrased Question']
                
                if args.retrival_type == "random":

                    defense_query, anchor_image_path, best_simiarity = retrival_defense_prompt(defense_prompt_pool,image_pool, retrival_type="random")
                    
                elif args.retrival_type == "sample-wise":

                    sample_embedding = get_clip_embedding(clip_model, clip_preprocess, full_image_path, query)
                    defense_query, anchor_image_path, best_simiarity = retrival_defense_prompt(defense_prompt_pool,image_pool, retrival_type= "sample-wise", sample_embedding=sample_embedding, embedding_pool=embedding_pool)

                qs = query + defense_query + query
            
                if model.config.mm_use_im_start_end:
                    qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs
                else:
                    qs = DEFAULT_IMAGE_TOKEN + '\n' + qs

                conv = conv_templates[conv_mode].copy()
                conv.append_message(conv.roles[0], qs)
                conv.append_message(conv.roles[1], None)
                prompt = conv.get_prompt()

                input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()


                image = Image.open(full_image_path)
                image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]

                stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
                keywords = [stop_str]
                stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)
                with torch.inference_mode():
                    output_ids = model.generate(
                        input_ids,
                        images=image_tensor.unsqueeze(0).half().cuda(),
                        do_sample=True,
                        temperature=args.temperature,
                        top_p=args.top_p,
                        num_beams=args.num_beams,
                        max_new_tokens=args.max_new_tokens,
                        use_cache=True)

                input_token_len = input_ids.shape[1]
                n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
                if n_diff_input_output > 0:
                    print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
                outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
                outputs = outputs.strip()
                if outputs.endswith(stop_str):
                    print('stop str')
                    outputs = outputs[:-len(stop_str)]
                ans_id = shortuuid.uuid()
                ans_file.write(json.dumps({"text": outputs,
                                "defense_query":defense_query,
                                "anchor_image_path":anchor_image_path,
                                "best_simiarity": best_simiarity,   
                                "image_file": full_image_path,
                                "answer_id": ans_id,
                                "model_id": model_name,
                                "metadata": {}}) + "\n")
                ans_file.flush()
            else:
                print("pass")

            par.update(1)
    ans_file.close()



def main(args):
    if args.target_model == "llava":
        Attack_LLaVA(args)

    elif args.target_model == "minigptv2":
        cudnn.benchmark = False
        cudnn.deterministic = True
        Attack_MiniGPT(args)

    elif args.target_model == "cogvlm":
        Attack_CogVLM(args)

    else:
        print("please check your model name")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Experiment arguments for VLM defense")

    ####################### The file path config of data and results ###############################
    parser.add_argument("--query_type", type=str, default="SD_TYPO", choices=["SD", "SD_TYPO" , "TYPO"])
    parser.add_argument("--scenario", type=str, default="01-Illegal_Activitiy",choices=[ "01-Illegal_Activitiy",
            "02-HateSpeech", "03-Malware_Generation", "04-Physical_Harm",
            "05-EconomicHarm", "06-Fraud", "07-Sex", "08-Political_Lobbying", 
            "09-Privacy_Violence", "10-Legal_Opinion", "11-Financial_Advice", "12-Health_Consultation", "13-Gov_Decision"
        ])

    parser.add_argument("--table_dir", type=str, default="wandb/llava")
    parser.add_argument("--wandb_subdir", type=str, default="best_v1")
    parser.add_argument("--result_path", type=str, default="results/result-safe1.jsonl")
    parser.add_argument("--data_split_file", type=str, default="data/datasplit/query_related.jsonl")
    parser.add_argument("--image_folder", type=str, default="data/QueryRelated")

    parser.add_argument("--prompt_dir", type=str, default="prompts/QueryRelated")

    ############################### retrieval configure ###############################
    parser.add_argument("--retrival_type", type=str, default="sample-wise") 

    parser.add_argument(
        "--target-model",
        default = "llava",
        help = "Name of target vision-language model.",
        choices=["cogvlm", "minigptv2", "llava"]
    )

    args = parser.parse_args()

    ############################### specific model config ###############################
    if args.target_model == "llava":
        args.model_path="modellib/llava-v1.5-13b"
        args.model_base=None
        args.conv_mode="vicuna_v1"
        args.sep=","
        args.temperature=0.2
        args.top_p=0.7
        args.num_beams=1
        args.max_new_tokens=3000

    elif args.target_model == "minigptv2":
        args.cfg_path="eval_config/minigptv2_eval.yaml"
        args.gpu_id=0
        args.options = None

    elif args.target_model == "cogvlm":
        args.model_path="modellib/cogvlm-chat-hf"
        args.llm_path="modellib/vicuna-7b-v1.5"
        args.model_base=None
        args.conv_mode=None
        args.sep=","
        args.temperature=0.8
        args.top_p=0.4
        args.top_k=1
        args.num_beams=1
        args.max_new_tokens=512
        args.max_length=3000

    main(args)
